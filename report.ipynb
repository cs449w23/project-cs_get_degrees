{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs449w23/project-cs_get_degrees/blob/main/report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Goals and discussion (16 points)##\n"
      ],
      "metadata": {
        "id": "cqGeBTxQVN_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Essential goals<h1>\n",
        "\n",
        "List all essential goals that you proposed either in your second proposal or project update. For each, (a) include one or two sentences on how you completed it, what progress you made, or why you abandoned it, and (b) include one to two sentences on what was the most interesting or difficult part.\n",
        "\n"
      ],
      "metadata": {
        "id": "gDKMmNVrVVeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We will design a CNN and use it to generate feature vectors for the x-ray scans.\n",
        "\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We used Pytorch’s Conv-2D to design a CNN that has 4 2D-convolutional layers, and a kernel size of 3 for all layers. The initial channel size can be specified using a keyword argument with the channel size being doubled 3 times leading to a final channel size of 8 * initial_channel_size. The most interesting part was understanding how the CNN learns information from the image at each layer as this enabled us to know what output size we expected at each layer. This was particularly useful as the output of the final conv-2D layer served as input to the rest of our models.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "-  We will then vary the output network between an RNN, LSTM and a fully connected feedforward neural network and benchmark the performances of all of them.\n",
        "\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We used Pytorch’s RNNCell, LSTMCell and Linear respectively to implement our 3 output models. For the RNN and LSTM, we used the cell implementations as we wanted to represent a single time step of the input as the sequential pixels and then concatenate the output of all the cells to make a prediction. We benchmarked the performance of the three models by training each set on 75000 data points and validating on 7500 data points. The most interesting part was understanding how the RNN and LSTM actually work and the intricate differences between both. As per theory, the LSTM outperformed the RNN.  \n",
        "\n",
        "<br></br>\n",
        "\n",
        "\n",
        "- Achieve at least 85% accuracy with one of our models\n",
        "\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We then computed and plotted the average best accuracies for the CNN-FC, CNN-RNN and CNN-LSTM models with accuracies ranging from 83.7% - 87.1%. The interesting and difficult part was hypothesizing the subtle differences between the performances of the three models. For example, we initially thought that the RNN would outperform the fully connected network but this was not the case and we came to the conclusion that the vanishing gradient problem of the RNN factored into this."
      ],
      "metadata": {
        "id": "nm4RH6L7Visy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Desired goals</h1>\n",
        "\n",
        "List all desired goals that you proposed either in your second proposal or project update. For each, (a) include one or two sentences on how you completed it, what progress you made, or why you abandoned it, and (b) include one to two sentences on what was the most interesting or difficult part.\n"
      ],
      "metadata": {
        "id": "cBf4PW96Wd5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We will conduct a hyperparameter search for the best performing architecture combinations and see how changes in these parameters affect the performance, with the intention of improving performance even more.\n",
        "\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For our hyperparameter search, we varied three parameters:initial channel size, learning rate and dropout, that we identified as having the most impact on our train and validation loss and accuracy. For each parameter that we varied, we plotted the losses and accuracies and compared the performance across all. The most interesting part was that the hyperparameters that worked one for one architecture seemed to work well for the other architectures as well. The difficult part was inferring what the effect of changing multiple hyperparameters while only having the resources to change one at a time..\n",
        "\n",
        "- Achieve at least 90% accuracy with one of our models \n",
        "\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;After identifying the best hyperparameters for the binary classification task : (batch_size = 256, initial_channel_size = 80, lr=1e-4 ,epochs=15, dropout = 0.3), we trained and validated our models with these hyperparameters. However, we were not able to meet our goal of at least 90% accuracy. Our best performing model was the CNN-LSTM which got to a highest validation accuracy of 87.13%. The most difficult part while trying to achieve a 90% accuracy was while doing the hyperparameter search as discussed above. Additionally, the team hypothesized that a larger model would be able to achieve a higher accuracy, however given lack of compute power and time we were not able to test this theory.\n",
        "\n",
        "- We will test our architecture on a multi-class classification task to see whether it yields the same results.\n",
        "\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We then trained our CNN-FC, CNN-RNN, CNN-LSTM best performing architectures on the NIH-Chest-Xray-dataset that has 15 possible classes. While we were able to modify our existing architecture to perform computation on the new dataset, we were only able to train a zero rate classifier. We think this is due to the complexity of the problem. The dataset is a multilabel, highly skewed dataset with high resolution images. While we were able to decrease the image resolution from 1024x1024 to 256x256 the other complications remained. We were able to train a model to memorize ~500 training examples over the course of 200 epochs. Despite this, we believe that our model did not have enough capacity to learn how to classify this dataset, moreover increasing the number of convolutional layers and increasing initial channel size was still insufficient. It may be the case that the team does not have adequate resources at this time to train a model that can perform well with the NIH-Chest-Xray-dataset. "
      ],
      "metadata": {
        "id": "6w-xLXRlWl1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Stretch goals<h1>\n",
        "\n",
        "List all stretch goals that you proposed either in your second proposal or project update. For each, (a) include one or two sentences on how you completed it, what progress you made, or why you abandoned it, and (b) include one to two sentences on what was the most interesting or difficult part.\n"
      ],
      "metadata": {
        "id": "LjpMreD8XF3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-Since a ResNet solves the vanishing gradient problem in deep CNNs, we will switch out the CNN for a ResNet (either design our own or use a pre-trained one) to generate the feature vectors.\n",
        "\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We abandoned this goal due to lack of applicability. ResNet does indeed solve the vanishing gradient problem, however after spending well over 45 hours of compute time on networks that were not large enough to warrant the use of a skip connection the lack of applicability in our case became clear. In the theoretical case where we are able to go further with our multiclass/multilabel goal, the ResNet could be applicable. However with regards to the work we were able to accomplish, the ResNet approach is not applicable. \n",
        "\n",
        "- We will then vary the output network the same way we did with the CNN and benchmark the performance.\n",
        "\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As discussed above, the limitation on training time did not allow us to implement and train the Resnet-{} architectures. As a result, we could not benchmark the performance.\n",
        "\n",
        "- Achieve at least 95% accuracy with one of our models (For reference)\n",
        "\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Similar to the above goal, we did not end up designing the Resnet-{} architectures because of the limitation on training time. \n"
      ],
      "metadata": {
        "id": "CZSjgj1cXKyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Everything else<h1>\n",
        "\n",
        "Discuss the additional work you did as part of this project that was not covered by the goals you set for yourself. Try to organize this into \"goals\" that you could have set for yourself if you had known what needed to be done. For each of these goals, write one or two sentences on why you focused on it, and one or two sentences on what was the most interesting or difficult part. This can be something practical like \"we couldn't load our data/model on Colab\" or something conceptual like \"we tried to understand the fancy model from this new paper but couldn't figure out how to implement it.\"\n"
      ],
      "metadata": {
        "id": "KuVkXhBwXnrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>PASS</h1>"
      ],
      "metadata": {
        "id": "qQKFWtFlXqha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code and documentation (10 points)##\n",
        "\n",
        "While you should not include code in this report, all your code should be uploaded to your GitHub repository. In this part of your report, you should list all .py or .ipynb files that contain your code and a one or two sentence description of what that file contains. Then, in each of those files, you should make sure that there is enough documentation that we can read through the repository and understand what each part of the code does. This code does not necessarily have to be in immediately working order (e.g., if you cannot upload your data or model files to GitHub), but we should be able to read through your code and notebooks to understand how you implemented things. You absolutely do not need to include a comment for every line of code, but you should include docstrings for (most of) the functions you wrote and descriptions for the coding cells within any .ipynb files.\n"
      ],
      "metadata": {
        "id": "3gpOBc5sXvsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| File      | Description |\n",
        "| ----------- | ----------- |\n",
        "| CNN.ipynb      | Ran binary classification experiments on the CNN-fully-connected, CNN-RNN and CNN-LSTM     |\n",
        "| CNN_multiclass.ipynb   | Ran multiclass classification experiments on the CNN-fully-connected, CNN-RNN and CNN-LSTM        |\n",
        "|Pre-process_X-ray.ipynb | For preprocessing the Chest X-ray images for the multi-label classification task |\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQeotXI-YAGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflections (6 points) ##\n",
        "This section is your chance to reflect on this project. You should write at least a few sentences for each of these questions. However, try to be concise; a longer answer is not necessarily a better answer. If you want to write several paragraphs about something you're excited about, that's great! On the other hand, don't just write several paragraphs listing all the hyperparameter values you tried; if we fall asleep reading your answers, you might lose points.\n"
      ],
      "metadata": {
        "id": "gxjGnQjkYn7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<strong><h3>What was interesting?</h3></strong>\n",
        "\n",
        "What did you learn from this project that wasn't covered by other parts of the class? \n",
        "\n",
        "* We learned how we could combine CNNs and RNNs to solve complex tasks. We also understood the importance of hyperparameter tuning and how making changes to these parameters can significantly affect the performance of the models. Lastly, exploring the limitations of the models was an eye-opening experience that highlighted the fact that sometimes, despite our best efforts, achieving the desired accuracy is not always possible.\n",
        "\n",
        "\n",
        "What concepts from the lectures and readings were most relevant to your project?\n",
        "\n",
        "* We found that the concepts we learned in the lectures and readings on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) models, and the paper on Batch Normalization were highly relevant. The CNNs helped us to effectively classify images by detecting relevant features, while the RNNs and LSTMs enabled us to model sequential data and make predictions based on previous inputs. Additionally, the use of Batch Normalization helped to improve the overall performance and convergence of the models during training by normalizing the input data. These concepts were integral to our project and helped us to achieve accurate and meaningful results.\n",
        "\n",
        "\n",
        "<strong><h3>What was difficult?</h3></strong>\n",
        "\n",
        "What were the hardest or most frustrating parts of this project? If someone were about to start on a similar project from scratch, what would you encourage them to do differently?\n",
        "\n",
        "* This project provided valuable insights into practical challenges that arise when working with machine learning models. One of the challenges was dealing with large datasets while having limited computing resources, which required finding creative solutions to optimize the training process. Another challenge was debugging errors in the code, which helped to develop a deeper understanding of the source of the problem and how to solve it. Finally, optimizing the training process to reduce training time was another important skill learned during the project. We would encourage people starting a similar project to begin by building smaller architectures than what they eventually aspire to build. This will enable the team to ramp up quite fast on the technology stack in question as in our case, Pytorch’s Conv2D, RNN and LSTM modules. Furthermore, it is easier to debug smaller architectures. Once a smaller architecture is working, they can then shift effort to building a bigger architecture. Furthermore, in the case of large datasets, you should start by testing and ensuring your models work on a smaller sample size. Finally, we would encourage them to regularly save models at the end of each training epoch/training session. When working with colab and kaggle sometimes the runtime environment would run out of memory and crash which meant that any previous work was lost. Regularly saving the models would ensure that the previous work can be restored easily.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<strong><h3>What's left to do?</h3></strong>\n",
        "\n",
        "If you were going to spend another month working full-time on this project, what would you try to accomplish? Why? If I gave your group 1,000,000 USD to use for data collection or compute resources, what would you spend it on? Why?\n",
        "\n",
        "* If given another month to work full-time on the project, we would aim to build bigger architectures, specifically including a ResNet for multiclass classification. This would allow us to find the best combination of architectures that yield the best accuracy, potentially leading to a significant breakthrough in medical X-ray image diagnosis. With 1,000,000 USD for data collection or compute resources, we would focus on optimizing the training process, investing in better hardware, and exploring more advanced deep learning techniques. By doing so, we hope to significantly reduce the training time, allowing us to train larger models and increase the accuracy of their results. Ultimately, these investments would enable the group to make more meaningful contributions to the medical field and improve the accuracy of X-ray image diagnosis for patients.\n",
        "\n"
      ],
      "metadata": {
        "id": "VM-T5zZTYuHF"
      }
    }
  ]
}